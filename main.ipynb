{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting note processing pipeline\n",
      "üîß Using model: gemma3:12b-it-qat\n",
      "üìÇ Input directory: /home/vikk/Documents/GitHub/College-Notes/Notes\n",
      "üìÇ Output directory: /home/vikk/Documents/GitHub/College-Notes/Structured_Notes\n",
      "\n",
      "üìÅ Processing: Engineering Chemistry/Chemistry.md\n",
      "  üîç Found 97 sections\n",
      "  ‚ö†Ô∏è  Skipping invalid section: ___\n",
      "  üìù Section 2: Greenhouse Effect and Global Warming\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43969/767119788.py:41: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  return response.json().get(\"response\", {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ü§ñ Generating AI content...\n",
      "    üíæ Saved to: Engineering-Chemistry/Chemistry/Greenhouse-Effect-and-Global-Warming.md\n",
      "  üìù Section 3: Scientific mechanisms behind the greenhouse effect\n",
      "    ü§ñ Generating AI content...\n",
      "    üíæ Saved to: Engineering-Chemistry/Chemistry/Scientific-mechanisms-behind-the-greenhouse-effect.md\n",
      "  üìù Section 4: Causes of increasing greenhouse gas concentrations\n",
      "    ü§ñ Generating AI content...\n",
      "    üíæ Saved to: Engineering-Chemistry/Chemistry/Causes-of-increasing-greenhouse-gas-concentrations.md\n",
      "  üìù Section 5: Current data and trends in global warming\n",
      "    ü§ñ Generating AI content...\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "# Helper Functions\n",
    "def sanitize_path_element(element: str) -> str:\n",
    "    \"\"\"Sanitize directory and filenames for all OSes\"\"\"\n",
    "    return re.sub(r'[^\\w\\-_\\.]', '', element.replace(' ', '-')).strip('-')\n",
    "\n",
    "def is_horizontal_rule(line: str) -> bool:\n",
    "    \"\"\"Check if a line is a Markdown horizontal rule\"\"\"\n",
    "    return bool(re.match(r'^[-*_]{3,}$', line.strip()))\n",
    "\n",
    "# Core Processing Functions\n",
    "def generate_metadata(content: str, model: str) -> Dict:\n",
    "    \"\"\"Generate metadata using local LLM\"\"\"\n",
    "    try:\n",
    "        if not content.strip():\n",
    "            return {}\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Analyze this text and extract key concepts following these rules:\n",
    "        1. Identify primary concept (PascalCase)\n",
    "        2. List 2-5 related concepts (PascalCase)\n",
    "        3. Generate 1-3 tags (lowercase-with-dashes)\n",
    "        4. Create a 1-sentence summary\n",
    "\n",
    "        Text: {content[:2000]}\n",
    "        \"\"\"\n",
    "\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            format=\"json\",\n",
    "            options={\"temperature\": 0.2}\n",
    "        )\n",
    "        return response.model_dump_json().get(\"response\", {})\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "def generate_ai_content(title: str, concepts: List[str], folder_hierarchy: List[str], model: str) -> str:\n",
    "    \"\"\"Generate content using AI with folder context\"\"\"\n",
    "    try:\n",
    "        context_path = \" > \".join(folder_hierarchy)\n",
    "        prompt = f\"\"\"\n",
    "        Generate comprehensive content for: {title}\n",
    "        Context Hierarchy: {context_path}\n",
    "        Include:\n",
    "        - Core definitions\n",
    "        - Practical applications\n",
    "        - Relationships to parent concepts\n",
    "        - Simple examples\n",
    "        Use academic tone with Markdown sections\n",
    "        \"\"\"\n",
    "\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={\"temperature\": 0.5}\n",
    "        )\n",
    "        return f\"> **AI Generated Content**\\n{response['response']}\"\n",
    "    except:\n",
    "        return \"> **AI Generation Failed** - Content placeholder\"\n",
    "\n",
    "def merge_moc_content(existing_content: str, new_moc: str) -> str:\n",
    "    \"\"\"Merge existing note content with MOC sections\"\"\"\n",
    "    if existing_content.startswith('---'):\n",
    "        frontmatter_end = existing_content.find('---', 3)\n",
    "        if frontmatter_end != -1:\n",
    "            frontmatter = existing_content[:frontmatter_end+3]\n",
    "            existing_content = existing_content[frontmatter_end+3:].lstrip()\n",
    "            existing_content = re.sub(r'##+ (Subtopics|Notes)[\\s\\S]*?(?=##|$)', '', existing_content).strip()\n",
    "            return f\"{frontmatter}\\n{existing_content}\\n\\n{new_moc}\"\n",
    "    return f\"{existing_content}\\n\\n{new_moc}\".strip()\n",
    "\n",
    "# Main Processing Pipeline\n",
    "def process_note_with_metadata(note_path: Path, output_dir: Path, model: str, notes_root: Path):\n",
    "    \"\"\"Process notes with full error handling and MOC merging\"\"\"\n",
    "    try:\n",
    "        rel_path = note_path.relative_to(notes_root)\n",
    "        print(f\"\\n\\U0001F4C1 Processing: {rel_path}\")\n",
    "\n",
    "        with open(note_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "\n",
    "        sections = re.split(r'\\n##+ ', content)\n",
    "        print(f\"  \\U0001F50D Found {len(sections)} sections\")\n",
    "\n",
    "        sanitized_hierarchy = [sanitize_path_element(p) for p in rel_path.parent.parts]\n",
    "        note_stem = sanitize_path_element(note_path.stem)\n",
    "\n",
    "        output_folder = output_dir.joinpath(*sanitized_hierarchy, note_stem)\n",
    "\n",
    "        if output_folder.exists() and output_folder.is_file():\n",
    "            new_name = f\"{note_stem}-{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "            output_folder = output_dir.joinpath(*sanitized_hierarchy, new_name)\n",
    "            print(f\"  ‚ö†Ô∏è  Renamed conflicting file to: {new_name}\")\n",
    "\n",
    "        output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        moc_entries = []\n",
    "        for i, section in enumerate(sections, 1):\n",
    "            if not section.strip():\n",
    "                continue\n",
    "\n",
    "            lines = section.split('\\n')\n",
    "            original_title = lines[0].strip('#').strip()\n",
    "\n",
    "            if is_horizontal_rule(original_title) or not original_title:\n",
    "                print(f\"  ‚ö†Ô∏è  Skipping invalid section: {original_title}\")\n",
    "                continue\n",
    "\n",
    "            body = '\\n'.join(lines[1:])\n",
    "            print(f\"  \\U0001F4DD Section {i}: {original_title}\")\n",
    "\n",
    "            try:\n",
    "                metadata = generate_metadata(f\"{original_title}\\n\\n{body}\", model)\n",
    "                concepts = metadata.get('concepts', [])\n",
    "                ai_generated = False\n",
    "\n",
    "                if not body.strip() or not re.search(r'^#+ ', body, flags=re.MULTILINE):\n",
    "                    print(\"    \\U0001F916 Generating AI content...\")\n",
    "                    body = generate_ai_content(original_title, concepts, sanitized_hierarchy, model)\n",
    "                    ai_generated = True\n",
    "\n",
    "                sanitized_name = sanitize_path_element(original_title)\n",
    "                output_path = output_folder / f\"{sanitized_name}.md\"\n",
    "                counter = 1\n",
    "                while output_path.exists():\n",
    "                    output_path = output_folder / f\"{sanitized_name}-{counter}.md\"\n",
    "                    counter += 1\n",
    "\n",
    "                frontmatter = {\n",
    "                    'created': datetime.now().isoformat(),\n",
    "                    'modified': datetime.now().isoformat(),\n",
    "                    'source': f\"[[{note_stem}]]\",\n",
    "                    'hierarchy': sanitized_hierarchy,\n",
    "                    'tags': metadata.get('tags', []),\n",
    "                    'summary': metadata.get('summary', ''),\n",
    "                    'concepts': concepts,\n",
    "                    'ai_generated': ai_generated\n",
    "                }\n",
    "                yaml_front = yaml.safe_dump(frontmatter, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "                note_content = f\"---\\n{yaml_front}---\\n\\n\"\n",
    "                note_content += f\"# {original_title}\\n\\n\"\n",
    "                note_content += f\"## Context Path\\n{' > '.join(sanitized_hierarchy)}\\n\\n\" if sanitized_hierarchy else \"\"\n",
    "                note_content += \"## Content\\n\" + body + \"\\n\\n\"\n",
    "                note_content += \"## Related Concepts\\n\" + '\\n'.join(f\"[[{c}]]\" for c in concepts[1:])\n",
    "\n",
    "                output_path.write_text(note_content, encoding='utf-8')\n",
    "                moc_entries.append(output_path)\n",
    "                print(f\"    \\U0001F4BE Saved to: {output_path.relative_to(output_dir)}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ùå Section processing failed: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        create_folder_moc(output_folder, moc_entries)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to process {note_path.name}: {str(e)}\")\n",
    "\n",
    "def create_folder_moc(folder_path: Path, entries: List[Path]):\n",
    "    \"\"\"Create/update MOC for each folder with merging\"\"\"\n",
    "    moc_name = f\"{folder_path.name} MOC.md\"\n",
    "    moc_path = folder_path / moc_name\n",
    "\n",
    "    existing_content = \"\"\n",
    "    if moc_path.exists():\n",
    "        try:\n",
    "            existing_content = moc_path.read_text(encoding='utf-8')\n",
    "            print(f\"    üîÑ Merging with existing MOC content\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ö†Ô∏è  Error reading MOC: {str(e)}\")\n",
    "\n",
    "    new_moc = []\n",
    "    subfolders = [f for f in folder_path.iterdir() if f.is_dir()]\n",
    "    if subfolders:\n",
    "        new_moc.append(\"## Subtopics\")\n",
    "        new_moc.extend(f\"- [[{f.name}/{f.name} MOC]]\" for f in subfolders)\n",
    "\n",
    "    notes = [f for f in folder_path.iterdir() \n",
    "             if f.is_file() and f != moc_path and f.suffix == '.md']\n",
    "    if notes:\n",
    "        new_moc.append(\"\\n## Notes\")\n",
    "        new_moc.extend(f\"- [[{f.stem.replace('-', ' ')}]]\" for f in notes)\n",
    "\n",
    "    final_content = f\"# {folder_path.name} Map of Content\\n\\n\" + '\\n'.join(new_moc)\n",
    "    if existing_content:\n",
    "        final_content = merge_moc_content(existing_content, '\\n'.join(new_moc))\n",
    "\n",
    "    if folder_path != folder_path.parent:\n",
    "        parent_moc = folder_path.parent / f\"{folder_path.parent.name} MOC.md\"\n",
    "        final_content += f\"\\n\\n‚§¥Ô∏è Back to [[{parent_moc.stem.replace('-', ' ')}]]\"\n",
    "\n",
    "    try:\n",
    "        moc_path.write_text(final_content, encoding='utf-8')\n",
    "        print(f\"    \\U0001F4D1 Updated MOC at: {moc_path.relative_to(folder_path.parent)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ‚ùå MOC creation failed: {str(e)}\")\n",
    "\n",
    "# Global Index and Main Execution\n",
    "def create_global_indices(output_dir: Path):\n",
    "    \"\"\"Create hierarchical indices\"\"\"\n",
    "    print(\"\\nüìö Building global indices...\")\n",
    "    index_content = \"# Knowledge Hierarchy Index\\n\\n\"\n",
    "\n",
    "    for path in sorted(output_dir.glob(\"**/* MOC.md\")):\n",
    "        if path.name.startswith('_'):\n",
    "            continue\n",
    "\n",
    "        relative_path = path.relative_to(output_dir)\n",
    "        depth = len(relative_path.parent.parts) - 1\n",
    "        indent = \"  \" * depth\n",
    "        display_name = ' '.join(relative_path.parent.name.split('-'))\n",
    "\n",
    "        index_content += f\"{indent}- [[{display_name} MOC]]\\n\"\n",
    "\n",
    "    index_path = output_dir / \"_HIERARCHY.md\"\n",
    "    index_path.write_text(index_content, encoding='utf-8')\n",
    "    print(f\"    üìñ Global index created at: {index_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    VAULT_ROOT = Path(\"/home/vikk/Documents/GitHub/College-Notes\")\n",
    "    NOTES_ROOT = VAULT_ROOT / \"Notes\"\n",
    "    OUTPUT_DIR = VAULT_ROOT / \"Structured_Notes\"\n",
    "    MODEL = \"gemma3:12b-it-qat\"\n",
    "\n",
    "    print(\"\\U0001F680 Starting note processing pipeline\")\n",
    "    print(f\"\\U0001F527 Using model: {MODEL}\")\n",
    "    print(f\"\\U0001F4C2 Input directory: {NOTES_ROOT}\")\n",
    "    print(f\"\\U0001F4C2 Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    total = sum(1 for _ in NOTES_ROOT.glob(\"**/*.md\") if \"MOC\" not in _.name)\n",
    "    processed = 0\n",
    "\n",
    "    for note_path in NOTES_ROOT.glob(\"**/*.md\"):\n",
    "        if \"MOC\" in note_path.name:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            process_note_with_metadata(note_path, OUTPUT_DIR, MODEL, NOTES_ROOT)\n",
    "            processed += 1\n",
    "            print(f\"‚úÖ Progress: {processed}/{total} ({processed/total:.1%})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Critical error processing {note_path}, skipping: {str(e)}\")\n",
    "\n",
    "    create_global_indices(OUTPUT_DIR)\n",
    "\n",
    "    print(f\"\\nüéâ Processing complete! Success: {processed}/{total} notes\")\n",
    "    print(f\"üåê Index available at: {OUTPUT_DIR}/_HIERARCHY.md\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
